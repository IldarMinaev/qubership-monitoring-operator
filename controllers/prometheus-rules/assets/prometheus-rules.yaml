apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    name: prometheus-rules
    app.kubernetes.io/name: prometheus-rules
    app.kubernetes.io/component: monitoring
    app.kubernetes.io/part-of: monitoring
    app.kubernetes.io/managed-by: monitoring-operator
  name: prometheus-rules
spec:
  groups:
  - name: Heartbeat
    rules:
    - alert: DeadMansSwitch
      expr: vector(1)
      for: 0s
      labels:
        severity: information
      annotations:
        summary: "An always-firing Dead Man's Switch alert (instance {{ $labels.instance }})"
        description: "This is an alert meant to ensure that the entire alerting pipeline is functional. This alert should always be firing.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - name: SelfMonitoring
    rules:
    - alert: PrometheusJobMissing
      expr: absent(up{job=~".*prometheus-pod-monitor"})
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Prometheus job missing (instance {{ $labels.instance }})"
        description: "A Prometheus job has disappeared\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PrometheusTargetMissing
      expr: up == 0
      for: 5m
      labels:
        severity: high
      annotations:
        summary: "Prometheus target missing (instance {{ $labels.instance }})"
        description: "A Prometheus target has disappeared. An exporter might be crashed.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PrometheusAllTargetsMissing
      expr: count by(job) (up) == count by(job) (up == 0)
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Prometheus all targets missing (job {{ $labels.job }})"
        description: "A Prometheus job does not have living target anymore.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PrometheusConfigurationReloadFailure
      expr: prometheus_config_last_reload_successful != 1
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Prometheus configuration reload failure (instance {{ $labels.instance }})"
        description: "Prometheus configuration reload error\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PrometheusTooManyRestarts
      expr: changes(process_start_time_seconds{job=~".*prometheus-pod-monitor"}[15m]) > 2
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Prometheus too many restarts (instance {{ $labels.instance }})"
        description: "Prometheus has restarted more than twice in the last 15 minutes. It might be crashlooping.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PrometheusRuleEvaluationFailures
      expr: increase(prometheus_rule_evaluation_failures_total[3m]) > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Prometheus rule evaluation failures (instance {{ $labels.instance }})"
        description: "Prometheus encountered {{ $value }} rule evaluation failures, leading to potentially ignored alerts.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PrometheusTemplateTextExpansionFailures
      expr: increase(prometheus_template_text_expansion_failures_total[3m]) > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Prometheus template text expansion failures (instance {{ $labels.instance }})"
        description: "Prometheus encountered {{ $value }} template text expansion failures\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PrometheusRuleEvaluationSlow
      expr: prometheus_rule_group_last_duration_seconds > prometheus_rule_group_interval_seconds
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Prometheus rule evaluation slow (instance {{ $labels.instance }})"
        description: "Prometheus rule evaluation took more time than the scheduled interval. I indicates a slower storage backend access or too complex query.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PrometheusNotificationsBacklog
      expr: min_over_time(prometheus_notifications_queue_length[10m]) > 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Prometheus notifications backlog (instance {{ $labels.instance }})"
        description: "The Prometheus notification queue has not been empty for 10 minutes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PrometheusTargetEmpty
      expr: prometheus_sd_discovered_targets == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Prometheus target empty (instance {{ $labels.instance }})"
        description: "Prometheus has no target in service discovery\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PrometheusTargetScrapingSlowTwoMinutes
      expr: (prometheus_target_interval_length_seconds{interval="2m0s", quantile="0.9"}) > 135
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Prometheus target scraping slow (instance {{ $labels.instance }}) for 2 minutes"
        description: "Prometheus is scraping exporters slowly\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PrometheusTargetScrapingSlowOneMinute
      expr: (prometheus_target_interval_length_seconds{interval="1m0s", quantile="0.9"}) > 70
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Prometheus target scraping slow (instance {{ $labels.instance }}) for 1 minute"
        description: "Prometheus is scraping exporters slowly\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PrometheusTargetScrapingSlowThirtySeconds
      expr: (prometheus_target_interval_length_seconds{interval="30s", quantile="0.9"}) > 35
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Prometheus target scraping slow (instance {{ $labels.instance }}) for 30 seconds"
        description: "Prometheus is scraping exporters slowly\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PrometheusLargeScrape
      expr: increase(prometheus_target_scrapes_exceeded_sample_limit_total[10m]) > 10
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Prometheus large scrape (instance {{ $labels.instance }})"
        description: "Prometheus has many scrapes that exceed the sample limit\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PrometheusTargetScrapeDuplicate
      expr: increase(prometheus_target_scrapes_sample_duplicate_timestamp_total[5m]) > 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Prometheus target scrape duplicate (instance {{ $labels.instance }})"
        description: "Prometheus has many samples rejected due to duplicate timestamps but different values\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PrometheusTsdbCheckpointCreationFailures
      expr: increase(prometheus_tsdb_checkpoint_creations_failed_total[3m]) > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Prometheus TSDB checkpoint creation failures (instance {{ $labels.instance }})"
        description: "Prometheus encountered {{ $value }} checkpoint creation failures\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PrometheusTsdbCheckpointDeletionFailures
      expr: increase(prometheus_tsdb_checkpoint_deletions_failed_total[3m]) > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Prometheus TSDB checkpoint deletion failures (instance {{ $labels.instance }})"
        description: "Prometheus encountered {{ $value }} checkpoint deletion failures\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PrometheusTsdbCompactionsFailed
      expr: increase(prometheus_tsdb_compactions_failed_total[3m]) > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Prometheus TSDB compactions failed (instance {{ $labels.instance }})"
        description: "Prometheus encountered {{ $value }} TSDB compactions failures\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PrometheusTsdbHeadTruncationsFailed
      expr: increase(prometheus_tsdb_head_truncations_failed_total[3m]) > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Prometheus TSDB head truncations failed (instance {{ $labels.instance }})"
        description: "Prometheus encountered {{ $value }} TSDB head truncation failures\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PrometheusTsdbReloadFailures
      expr: increase(prometheus_tsdb_reloads_failures_total[3m]) > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Prometheus TSDB reload failures (instance {{ $labels.instance }})"
        description: "Prometheus encountered {{ $value }} TSDB reload failures\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PrometheusTsdbWalCorruptions
      expr: increase(prometheus_tsdb_wal_corruptions_total[3m]) > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Prometheus TSDB WAL corruptions (instance {{ $labels.instance }})"
        description: "Prometheus encountered {{ $value }} TSDB WAL corruptions\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PrometheusTsdbWalTruncationsFailed
      expr: increase(prometheus_tsdb_wal_truncations_failed_total[3m]) > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Prometheus TSDB WAL truncations failed (instance {{ $labels.instance }})"
        description: "Prometheus encountered {{ $value }} TSDB WAL truncation failures\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - name: AlertManager
    rules:
      - alert: PrometheusAlertmanagerConfigurationReloadFailure
        expr: alertmanager_config_last_reload_successful != 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Prometheus AlertManager configuration reload failure (instance {{ $labels.instance }})"
          description: "AlertManager configuration reload error\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: PrometheusNotConnectedToAlertmanager
        expr: prometheus_notifications_alertmanagers_discovered < 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Prometheus not connected to alertmanager (instance {{ $labels.instance }})"
          description: "Prometheus cannot connect the alertmanager\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: PrometheusAlertmanagerNotificationFailing
        expr: rate(alertmanager_notifications_failed_total[2m]) > 0
        for: 5m
        labels:
          severity: high
        annotations:
          summary: "Prometheus AlertManager notification failing (instance {{ $labels.instance }})"
          description: "Alertmanager is failing sending notifications\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - name: KubebernetesAlerts
    rules:
    - alert: KubernetesNodeReady
      expr: kube_node_status_condition{condition="Ready",status="true"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes Node ready (instance {{ $labels.instance }})"
        description: "Node {{ $labels.node }} has been unready for a long time\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesMemoryPressure
      expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes memory pressure (instance {{ $labels.instance }})"
        description: "{{ $labels.node }} has MemoryPressure condition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesDiskPressure
      expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes disk pressure (instance {{ $labels.instance }})"
        description: "{{ $labels.node }} has DiskPressure condition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesOutOfDisk
      expr: kube_node_status_condition{condition="OutOfDisk",status="true"} == 1
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes out of disk (instance {{ $labels.instance }})"
        description: "{{ $labels.node }} has OutOfDisk condition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesJobFailed
      expr: kube_job_status_failed > 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes Job failed (instance {{ $labels.instance }})"
        description: "Job {{$labels.namespace}}/{{$labels.exported_job}} failed to complete\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesCronjobSuspended
      expr: kube_cronjob_spec_suspend != 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes CronJob suspended (instance {{ $labels.instance }})"
        description: "CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is suspended\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesPersistentvolumeclaimPending
      expr: kube_persistentvolumeclaim_status_phase{phase="Pending"} == 1
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes PersistentVolumeClaim pending (instance {{ $labels.instance }})"
        description: "PersistentVolumeClaim {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is pending\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesPersistentvolumeError
      expr: (kube_persistentvolume_status_phase{phase=~"Failed|Pending",job="kube-state-metrics"}) > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes PersistentVolume error (instance {{ $labels.instance }})"
        description: "Persistent volume is in bad state\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesVolumeOutOfDiskSpaceWarning
      expr: (kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes) * 100 < 25
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: Kubernetes Volume out of disk space (instance {{ $labels.instance }})
        description: "Volume is almost full (< 25% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesVolumeOutOfDiskSpaceHigh
      expr: (kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes) * 100 < 10
      for: 2m
      labels:
        severity: high
      annotations:
        summary: Kubernetes Volume out of disk space (instance {{ $labels.instance }})
        description: "Volume is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesVolumeFullInFourDays
      expr: predict_linear(kubelet_volume_stats_available_bytes[6h], 345600) < 0
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: Kubernetes Volume full in four days (instance {{ $labels.instance }})
        description: "{{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is expected to fill up within four days. Currently {{ $value | humanize }}% is available.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesStatefulsetDown
      expr: kube_statefulset_replicas - kube_statefulset_status_replicas_ready != 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes StatefulSet down (instance {{ $labels.instance }})"
        description: "A StatefulSet went down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesPodNotHealthy
      expr: min_over_time(sum by (exported_namespace, exported_pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"})[1h:1m]) > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes Pod not healthy (instance {{ $labels.instance }})"
        description: "Pod has been in a non-ready state for longer than an hour.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesPodCrashLooping
      expr: (rate(kube_pod_container_status_restarts_total[15m]) * 60) * 5 > 5
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes pod crash looping (instance {{ $labels.instance }})"
        description: "Pod {{ $labels.pod }} is crash looping\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesReplicassetMismatch
      expr: kube_replicaset_spec_replicas - kube_replicaset_status_ready_replicas != 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes ReplicasSet mismatch (instance {{ $labels.instance }})"
        description: "Deployment Replicas mismatch\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesDeploymentReplicasMismatch
      expr: kube_deployment_spec_replicas - kube_deployment_status_replicas_available != 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes Deployment replicas mismatch (instance {{ $labels.instance }})"
        description: "Deployment Replicas mismatch\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesStatefulsetReplicasMismatch
      expr: kube_statefulset_status_replicas_ready - kube_statefulset_status_replicas != 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes StatefulSet replicas mismatch (instance {{ $labels.instance }})"
        description: "A StatefulSet has not matched the expected number of replicas for longer than 15 minutes.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesDeploymentGenerationMismatch
      expr: kube_deployment_status_observed_generation - kube_deployment_metadata_generation != 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes Deployment generation mismatch (instance {{ $labels.instance }})"
        description: "A Deployment has failed but has not been rolled back.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesStatefulsetGenerationMismatch
      expr: kube_statefulset_status_observed_generation - kube_statefulset_metadata_generation != 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes StatefulSet generation mismatch (instance {{ $labels.instance }})"
        description: "A StatefulSet has failed but has not been rolled back.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesStatefulsetUpdateNotRolledOut
      expr: max without (revision) (kube_statefulset_status_current_revision unless kube_statefulset_status_update_revision) * (kube_statefulset_replicas != kube_statefulset_status_replicas_updated)
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes StatefulSet update not rolled out (instance {{ $labels.instance }})"
        description: "StatefulSet update has not been rolled out.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesDaemonsetRolloutStuck
      expr: (((kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled) * 100) < 100) or (kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled > 0)
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes DaemonSet rollout stuck (instance {{ $labels.instance }})"
        description: "Some Pods of DaemonSet are not scheduled or not ready\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesDaemonsetMisscheduled
      expr: kube_daemonset_status_number_misscheduled > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes DaemonSet misscheduled (instance {{ $labels.instance }})"
        description: "Some DaemonSet Pods are running where they are not supposed to run\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesCronjobTooLong
      expr: time() - kube_cronjob_next_schedule_time > 3600
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes CronJob too long (instance {{ $labels.instance }})"
        description: "CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is taking more than 1h to complete.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesJobCompletion
      expr: (kube_job_spec_completions - kube_job_status_succeeded > 0) or (kube_job_status_failed > 0)
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes job completion (instance {{ $labels.instance }})"
        description: "Kubernetes Job failed to complete\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesApiServerErrors
      expr: (sum(rate(apiserver_request_count{job="kube-apiserver",code=~"(?:5..)$"}[2m])) / sum(rate(apiserver_request_count{job="kube-apiserver"}[2m]))) * 100 > 3
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes API server errors (instance {{ $labels.instance }})"
        description: "Kubernetes API server is experiencing high error rate\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: ApiServerRequestsSlow
      expr: histogram_quantile(0.99, rate(apiserver_request_duration_seconds_bucket{verb!="WATCH"}[5m])) > 0.5
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "API Server requests are slow(instance {{ $labels.instance }})"
        description: "HTTP requests slowing down, 99th quantile is over 0.5s for 5 minutes\\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: ControllerWorkQueueDepth
      expr: sum(workqueue_depth) > 10
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Controller work queue depth is more than 10 (instance {{ $labels.instance }})"
        description: "Controller work queue depth is more than 10\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesApiClientErrors
      expr: (sum(rate(rest_client_requests_total{code=~"(4|5).."}[2m])) by (instance, job) / sum(rate(rest_client_requests_total[2m])) by (instance, job)) * 100 > 5
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes API client errors (instance {{ $labels.instance }})"
        description: "Kubernetes API client is experiencing high error rate\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesClientCertificateExpiresNextWeek
      expr: (apiserver_client_certificate_expiration_seconds_count{job="kubelet"}) > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="kubelet"}[5m]))) < 604800
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes client certificate expires next week (instance {{ $labels.instance }})"
        description: "A client certificate used to authenticate to the apiserver is expiring next week.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesClientCertificateExpiresSoon
      expr: (apiserver_client_certificate_expiration_seconds_count{job="kubelet"}) > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="kubelet"}[5m]))) < 86400
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetes client certificate expires soon (instance {{ $labels.instance }})"
        description: "A client certificate used to authenticate to the apiserver is expiring in less than 24.0 hours.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - name: NodeProcesses
    rules:
    - alert: CountPidsAndThreadOutOfLimit
      expr: (sum(container_processes) by (node) +  on (node) label_replace(node_processes_threads * on(instance) group_left(nodename) (node_uname_info), "node", "$1", "nodename", "(.+)")) / on (node) label_replace(node_processes_max_processes * on(instance) group_left(nodename) (node_uname_info), "node", "$1", "nodename", "(.+)") * 100 > 80
      for: 5m
      labels:
        severity: high
      annotations:
        summary: "Host high PIDs and Threads usage (instance {{ $labels.instance }})"
        description: "Sum of node's pids and threads is filling up (< 20% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - name: NodeExporters
    rules:
    - alert: NodeDiskUsageIsMoreThanThreshold
      annotations:
        description: "Node {{ $labels.node }} disk usage of {{ $labels.mountpoint }} is\n  VALUE = {{ $value }}%"
        summary: "Disk usage on node > 70% (instance {{ $labels.node }})"
      expr: (node_filesystem_size_bytes{fstype=~"ext.*|xfs", mountpoint !~".*pod.*"} - node_filesystem_free_bytes{fstype=~"ext.*|xfs", mountpoint !~".*pod.*"}) * 100 / (node_filesystem_avail_bytes{fstype=~"ext.*|xfs", mountpoint !~".*pod.*"} + (node_filesystem_size_bytes{fstype=~"ext.*|xfs", mountpoint !~".*pod.*"} - node_filesystem_free_bytes{fstype=~"ext.*|xfs", mountpoint !~".*pod.*"})) > 70
      for: 5m
      labels:
        severity: warning

    - alert: NodeDiskUsageIsMoreThanThreshold
      annotations:
        description: "Node {{ $labels.node }} disk usage of {{ $labels.mountpoint }} is\n VALUE = {{ $value }}%"
        summary: "Disk usage on node > 90% (instance {{ $labels.node }})"
      expr: (node_filesystem_size_bytes{fstype=~"ext.*|xfs", mountpoint !~".*pod.*"} - node_filesystem_free_bytes{fstype=~"ext.*|xfs", mountpoint !~".*pod.*"}) * 100 / (node_filesystem_avail_bytes{fstype=~"ext.*|xfs", mountpoint !~".*pod.*"} + (node_filesystem_size_bytes{fstype=~"ext.*|xfs", mountpoint !~".*pod.*"} - node_filesystem_free_bytes{fstype=~"ext.*|xfs", mountpoint !~".*pod.*"})) > 90
      for: 5m
      labels:
        severity: high

    - alert: HostOutOfMemory
      expr: ((node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100) * on(instance) group_left(nodename) node_uname_info < 10
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Host out of memory (instance {{ $labels.instance }})"
        description: "Node memory is filling up (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HostMemoryUnderMemoryPressure
      expr: rate(node_vmstat_pgmajfault[2m]) * on(instance) group_left(nodename) node_uname_info > 1000
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Host memory under memory pressure (instance {{ $labels.instance }})"
        description: "The node is under heavy memory pressure. High rate of major page faults\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HostUnusualNetworkThroughputIn
      expr: ((sum by (instance) (irate(node_network_receive_bytes_total[2m])) * on(instance) group_left(nodename) node_uname_info) / 1024) / 1024 > 100
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Host unusual network throughput in (instance {{ $labels.instance }})"
        description: "Host network interfaces are probably receiving too much data (> 100 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HostUnusualNetworkThroughputOut
      expr: ((sum by (instance) (irate(node_network_transmit_bytes_total[2m])) * on(instance) group_left(nodename) node_uname_info) / 1024) / 1024 > 100
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Host unusual network throughput out (instance {{ $labels.instance }})"
        description: "Host network interfaces are probably sending too much data (> 100 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HostUnusualDiskReadRate
      expr: (sum by (instance) (irate(node_disk_read_bytes_total[2m])) * on(instance) group_left(nodename) node_uname_info) / 1024 / 1024 > 50
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Host unusual disk read rate (instance {{ $labels.instance }})"
        description: "Disk is probably reading too much data (> 50 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HostUnusualDiskWriteRate
      expr: ((sum by (instance) (irate(node_disk_written_bytes_total[2m])) * on(instance) group_left(nodename) node_uname_info) / 1024) / 1024 > 50
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Host unusual disk write rate (instance {{ $labels.instance }})"
        description: "Disk is probably writing too much data (> 50 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HostOutOfDiskSpace
      expr: ((node_filesystem_avail_bytes{mountpoint="/"}  * 100) / node_filesystem_size_bytes{mountpoint="/"}) * on(instance) group_left(nodename) node_uname_info < 10
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Host out of disk space (instance {{ $labels.instance }})"
        description: "Disk is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HostDiskWillFillIn4Hours
      expr: predict_linear(node_filesystem_free_bytes{fstype!~"tmpfs"}[1h], 14400) * on(instance) group_left(nodename) node_uname_info < 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Host disk will fill in 4 hours (instance {{ $labels.instance }})"
        description: "Disk will fill in 4 hours at current write rate\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HostOutOfInodes
      expr: ((node_filesystem_files_free{mountpoint ="/"} / node_filesystem_files{mountpoint ="/"}) * 100) * on(instance) group_left(nodename) node_uname_info < 10
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Host out of inodes (instance {{ $labels.instance }})"
        description: "Disk is almost running out of available inodes (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HostUnusualDiskReadLatency
      expr: (rate(node_disk_read_time_seconds_total[2m]) / rate(node_disk_reads_completed_total[2m])) * on(instance) group_left(nodename) node_uname_info > 100
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Host unusual disk read latency (instance {{ $labels.instance }})"
        description: "Disk latency is growing (read operations > 100ms)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HostUnusualDiskWriteLatency
      expr: (rate(node_disk_write_time_seconds_total[2m]) / rate(node_disk_writes_completed_total[2m])) * on(instance) group_left(nodename) node_uname_info > 100
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Host unusual disk write latency (instance {{ $labels.instance }})"
        description: "Disk latency is growing (write operations > 100ms)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HostHighCpuLoad
      expr: 100 - ((avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) by (instance) * 100) * on (instance) group_left (nodename) node_uname_info) > 80
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Host high CPU load (instance {{ $labels.instance }})"
        description: "CPU load is > 80%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - name: DockerContainers
    rules:
    - alert: ContainerKilled
      expr: time() - container_last_seen > 60
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Container killed (instance {{ $labels.instance }})"
        description: "A container has disappeared\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: ContainerVolumeUsage
      expr: (1 - (sum(container_fs_inodes_free) BY (node) / sum(container_fs_inodes_total) BY (node))) * 100 > 80
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Container Volume usage (instance {{ $labels.instance }})"
        description: "Container Volume usage is above 80%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: ContainerVolumeIoUsage
      expr: (sum(container_fs_io_current) BY (node, name) * 100) > 80
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Container Volume IO usage (instance {{ $labels.instance }})"
        description: "Container Volume IO usage is above 80%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: ContainerHighThrottleRate
      expr: rate(container_cpu_cfs_throttled_seconds_total[3m]) > 1
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Container high throttle rate (instance {{ $labels.instance }})"
        description: "Container is being throttled\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - name: HAmode
    rules:
      - alert: NotHAKubernetesDeploymentAvailableReplicas
        expr: kube_deployment_status_replicas_available < 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Not HA mode: Deployment Available Replicas < 2 (instance {{ $labels.instance }})"
          description: "Not HA mode: Kubernetes Deployment has less than 2 available replicas\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: NotHAKubernetesStatefulSetAvailableReplicas
        expr: kube_statefulset_status_replicas_available < 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Not HA mode: StatefulSet Available Replicas < 2 (instance {{ $labels.instance }})"
          description: "Not HA mode: Kubernetes StatefulSet has less than 2 available replicas\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: NotHAKubernetesDeploymentDesiredReplicas
        expr: kube_deployment_status_replicas < 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Not HA mode: Deployment Desired Replicas < 2 (instance {{ $labels.instance }})"
          description: "Not HA mode: Kubernetes Deployment has less than 2 desired replicas\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: NotHAKubernetesStatefulSetDesiredReplicas
        expr: kube_statefulset_status_replicas < 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Not HA mode: StatefulSet Desired Replicas < 2 (instance {{ $labels.instance }})"
          description: "Not HA mode: Kubernetes StatefulSet has less than 2 desired replicas\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: NotHAKubernetesDeploymentMultiplePodsPerNode
        expr: count(sum(kube_pod_info{node=~".+", created_by_kind="ReplicaSet"}) by (namespace, node, created_by_name) > 1) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Not HA mode: Deployment Has Multiple Pods per Node (instance {{ $labels.instance }})"
          description: "Not HA mode: Kubernetes Deployment has 2 or more replicas on the same node\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: NotHAKubernetesStatefulSetMultiplePodsPerNode
        expr: count(sum(kube_pod_info{node=~".+", created_by_kind="StatefulSet"}) by (namespace, node, created_by_name) > 1) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Not HA mode: StatefulSet Has Multiple Pods per Node (instance {{ $labels.instance }})"
          description: "Not HA mode: Kubernetes StatefulSet has 2 or more replicas on the same node\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - name: HAproxy
    rules:
    - alert: HaproxyDown
      expr: haproxy_up == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "HAProxy down (instance {{ $labels.instance }})"
        description: "HAProxy down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HaproxyBackendConnectionErrors
      expr: sum by (backend) (rate(haproxy_backend_connection_errors_total[2m])) > 10
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "HAProxy backend connection errors (instance {{ $labels.instance }})"
        description: "Too many connection errors to {{ $labels.fqdn }}/{{ $labels.backend }} backend (> 10 req/s). Request throughput may be to high.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HaproxyServerResponseErrors
      expr: sum by (server) (rate(haproxy_server_response_errors_total[2m])) > 5
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "HAProxy server response errors (instance {{ $labels.instance }})"
        description: "Too many response errors to {{ $labels.server }} server (> 5 req/s).\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HaproxyServerConnectionErrors
      expr: sum by (server) (rate(haproxy_server_connection_errors_total[2m])) > 10
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "HAProxy server connection errors (instance {{ $labels.instance }})"
        description: "Too many connection errors to {{ $labels.server }} server (> 10 req/s). Request throughput may be to high.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HaproxyPendingRequests
      expr: sum by (backend) (haproxy_backend_current_queue) > 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "HAProxy pending requests (instance {{ $labels.instance }})"
        description: "Some HAProxy requests are pending on {{ $labels.fqdn }}/{{ $labels.backend }} backend\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HaproxyHttpSlowingDown
      expr: avg by (backend) (haproxy_backend_http_total_time_average_seconds) > 2
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "HAProxy HTTP slowing down (instance {{ $labels.instance }})"
        description: "Average request time is increasing\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HaproxyRetryHigh
      expr: sum by (backend) (rate(haproxy_backend_retry_warnings_total[5m])) > 10
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "HAProxy retry high (instance {{ $labels.instance }})"
        description: "High rate of retry on {{ $labels.fqdn }}/{{ $labels.backend }} backend\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HaproxyBackendDown
      expr: haproxy_backend_up == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "HAProxy backend down (instance {{ $labels.instance }})"
        description: "HAProxy backend is down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HaproxyServerDown
      expr: haproxy_server_up == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "HAProxy server down (instance {{ $labels.instance }})"
        description: "HAProxy server is down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HaproxyFrontendSecurityBlockedRequests
      expr: sum by (frontend) (rate(haproxy_frontend_requests_denied_total[5m])) > 10
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "HAProxy frontend security blocked requests (instance {{ $labels.instance }})"
        description: "HAProxy is blocking requests for security reason\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HaproxyServerHealthcheckFailure
      expr: increase(haproxy_server_check_failures_total[5m]) > 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "HAProxy server healthcheck failure (instance {{ $labels.instance }})"
        description: "Some server healthcheck are failing on {{ $labels.server }}\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - name: Etcd
    rules:
    - alert: EtcdInsufficientMembers
      expr: count(etcd_server_id{job="etcd"}) % 2 == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Etcd insufficient Members (instance {{ $labels.instance }})"
        description: "Etcd cluster should have an odd number of members\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: EtcdNoLeader
      expr: etcd_server_has_leader == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Etcd no Leader (instance {{ $labels.instance }})"
        description: "Etcd cluster have no leader\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: EtcdHighNumberOfLeaderChanges
      expr: increase(etcd_server_leader_changes_seen_total[1h]) > 3
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Etcd high number of leader changes (instance {{ $labels.instance }})"
        description: "Etcd leader changed more than 3 times during last hour\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: EtcdHighNumberOfFailedGrpcRequests
      expr: sum(rate(grpc_server_handled_total{job="etcd",grpc_code!="OK", grpc_method!="Watch"}[5m])) BY (grpc_service, grpc_method) / sum(rate(grpc_server_handled_total{job="etcd"}[5m])) BY (grpc_service, grpc_method) > 0.01
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Etcd high number of failed GRPC requests (instance {{ $labels.instance }})"
        description: "More than 1% GRPC request failure detected in Etcd for 5 minutes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: EtcdHighNumberOfFailedGrpcRequests
      expr: sum(rate(grpc_server_handled_total{job="etcd",grpc_code!="OK", grpc_method!="Watch"}[5m])) BY (grpc_service, grpc_method) / sum(rate(grpc_server_handled_total{job="etcd"}[5m])) BY (grpc_service, grpc_method) > 0.05
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Etcd high number of failed GRPC requests (instance {{ $labels.instance }})"
        description: "More than 5% GRPC request failure detected in Etcd for 5 minutes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: EtcdGrpcRequestsSlow
      expr: histogram_quantile(0.99, sum(rate(grpc_server_handling_seconds_bucket{job="etcd",grpc_type="unary"}[5m])) by (grpc_service, grpc_method, le)) > 0.15
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Etcd GRPC requests slow (instance {{ $labels.instance }})"
        description: "GRPC requests slowing down, 99th percentil is over 0.15s for 5 minutes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: EtcdMemberCommunicationSlow
      expr: histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket{job="etcd"}[5m])) > 0.15
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Etcd member communication slow (instance {{ $labels.instance }})"
        description: "Etcd member communication slowing down, 99th percentil is over 0.15s for 5 minutes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: EtcdHighNumberOfFailedProposals
      expr: increase(etcd_server_proposals_failed_total[1h]) > 5
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Etcd high number of failed proposals (instance {{ $labels.instance }})"
        description: "Etcd server got more than 5 failed proposals past hour\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: EtcdHighFsyncDurations
      expr: histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m])) > 0.5
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Etcd high fsync durations (instance {{ $labels.instance }})"
        description: "Etcd WAL fsync duration increasing, 99th percentil is over 0.5s for 5 minutes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: EtcdHighCommitDurations
      expr: histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket[5m])) > 0.25
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Etcd high commit durations (instance {{ $labels.instance }})"
        description: "Etcd commit duration increasing, 99th percentil is over 0.25s for 5 minutes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - name: NginxIngressAlerts
    rules:
    - alert: NginxHighHttp4xxErrorRate
      expr: sum by (ingress, exported_namespace, node) (rate(nginx_ingress_controller_requests{status=~"^4.."}[2m])) / sum by (ingress, exported_namespace, node)(rate(nginx_ingress_controller_requests[2m])) * 100 > 5
      for: 1m
      labels:
        severity: high
      annotations:
        summary: "Nginx high HTTP 4xx error rate (node: {{ $labels.node }}, namespace: {{ $labels.exported_namespace }}, ingress: {{ $labels.ingress }})"
        description: "Too many HTTP requests with status 4xx (> 5%)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

    - alert: NginxHighHttp5xxErrorRate
      expr: sum by (ingress, exported_namespace, node) (rate(nginx_ingress_controller_requests{status=~"^5.."}[2m])) / sum by (ingress, exported_namespace, node) (rate(nginx_ingress_controller_requests[2m])) * 100 > 5
      for: 1m
      labels:
        severity: high
      annotations:
        summary: "Nginx high HTTP 5xx error rate (node: {{ $labels.node }}, namespace: {{ $labels.exported_namespace }}, ingress: {{ $labels.ingress }})"
        description: "Too many HTTP requests with status 5xx (> 5%)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

    - alert: NginxLatencyHigh
      expr: histogram_quantile(0.99, sum(rate(nginx_ingress_controller_request_duration_seconds_bucket[2m])) by (host, node, le)) > 3
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Nginx latency high (node: {{ $labels.node }}, host: {{ $labels.host }})"
        description: "Nginx p99 latency is higher than 3 seconds\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

  - name: CoreDnsAlerts
    rules:
    - alert: CorednsPanicCount
      expr: increase(coredns_panics_total[1m]) > 0
      for: 0m
      labels:
        severity: critical
      annotations:
        summary: CoreDNS Panic Count (instance {{ $labels.instance }})
        description: "Number of CoreDNS panics encountered\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
    - alert: CoreDNSLatencyHigh
      annotations:
        description: CoreDNS has 99th percentile latency of {{ $value }} seconds for server {{ $labels.server }} zone {{ $labels.zone }}
        summary: CoreDNS have High Latency
      expr: histogram_quantile(0.99, sum(rate(coredns_dns_request_duration_seconds_bucket[2m])) by(server, zone, le)) > 3
      for: 5m
      labels:
        severity: critical

    - alert: CoreDNSForwardHealthcheckFailureCount
      annotations:
        summary: CoreDNS health checks have failed to upstream server
        description: CoreDNS health checks have failed to upstream server {{ $labels.to }}
      expr: sum(rate(coredns_forward_healthcheck_broken_total[2m])) > 0
      for: 5m
      labels:
        severity: warning

    - alert: CoreDNSForwardHealthcheckBrokenCount
      annotations:
        summary: CoreDNS health checks have failed for all upstream servers
        description: "CoreDNS health checks failed for all upstream servers LABELS = {{ $labels }}"
      expr: sum(rate(coredns_forward_healthcheck_broken_total[2m])) > 0
      for: 5m
      labels:
        severity: warning

    - alert: CoreDNSErrorsHigh
      annotations:
        description: CoreDNS is returning SERVFAIL for {{ $value | humanizePercentage }} of requests
        summary: CoreDNS is returning SERVFAIL
      expr: sum(rate(coredns_dns_responses_total{rcode="SERVFAIL"}[2m])) / sum(rate(coredns_dns_responses_total[2m])) > 0.03
      for: 5m
      labels:
        severity: critical
      
    - alert: CoreDNSErrorsHigh
      annotations:
        description: CoreDNS is returning SERVFAIL for {{ $value | humanizePercentage }} of requests
        summary: CoreDNS is returning SERVFAIL
      expr: sum(rate(coredns_dns_responses_total{rcode="SERVFAIL"}[2m])) / sum(rate(coredns_dns_responses_total[2m])) > 0.01
      for: 5m
      labels:
        severity: warning

    - alert: CoreDNSForwardLatencyHigh
      annotations:
        description: CoreDNS has 99th percentile latency of {{ $value }} seconds forwarding requests to {{ $labels.to }}
        summary: CoreDNS has 99th percentile latency for forwarding requests
      expr: histogram_quantile(0.99, sum(rate(coredns_forward_request_duration_seconds_bucket[2m])) by(to, le)) > 3
      for: 5m
      labels:
        severity: critical
      
    - alert: CoreDNSForwardErrorsHigh
      annotations:
        description: CoreDNS is returning SERVFAIL for {{ $value | humanizePercentage }} of forward requests to {{ $labels.to }}
        summary: CoreDNS is returning SERVFAIL for forward requests
      expr: sum(rate(coredns_forward_responses_total{rcode="SERVFAIL"}[2m])) / sum(rate(coredns_forward_responses_total[2m])) > 0.03
      for: 5m
      labels:
        severity: critical
      
      
    - alert: CoreDNSForwardErrorsHigh
      annotations:
        description: CoreDNS is returning SERVFAIL for {{ $value | humanizePercentage }} of forward requests to {{ $labels.to }}
        summary: CoreDNS is returning SERVFAIL for forward requests
      expr: sum(rate(coredns_forward_responses_total{rcode="SERVFAIL"}[2m])) / sum(rate(coredns_forward_responses_total[2m])) > 0.01
      for: 5m
      labels:
        severity: warning

  - name: DRAlerts
    rules:
      - alert: ProbeFailed
        expr: probe_success == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Probe failed (instance: {{ $labels.instance }})"
          description: "Probe failed\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: SlowProbe
        expr: avg_over_time(probe_duration_seconds[1m]) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Slow probe (instance: {{ $labels.instance }})"
          description: "Blackbox probe took more than 1s to complete\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: HttpStatusCode
        expr: probe_http_status_code <= 199 OR probe_http_status_code >= 400
        for: 5m
        labels:
          severity: high
        annotations:
          summary: "HTTP Status Code (instance: {{ $labels.instance }})"
          description: "HTTP status code is not 200-399\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: HttpSlowRequests
        expr: avg_over_time(probe_http_duration_seconds[1m]) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "HTTP slow requests (instance: {{ $labels.instance }})"
          description: "HTTP request took more than 1s\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

  - name: BackupAlerts
    rules:
      - alert: Last Backup Failed
        expr: backup_storage_last_failed != 0
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Last backup made by pod {{ $labels.pod }} in namespace {{ $labels.namespace }} failed.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          description: "Last backup made by pod {{ $labels.pod }} in namespace {{ $labels.namespace }} failed.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"